{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduccion de Dimensionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Cargando etiquetas...\n",
      "‚öôÔ∏è Aplicando PCA en lotes con `IncrementalPCA`...\n",
      "üì¶ Procesando PCA - Lote 1/41...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 123/250 [00:00<00:00, 148.44it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.09 MiB for an array with shape (810000,) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(PREPROCESSED_PATH, image_name)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(image_path):\n\u001b[1;32m---> 45\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     X_batch\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[0;32m     47\u001b[0m     y_batch\u001b[38;5;241m.\u001b[39mappend(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiagnosis\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.09 MiB for an array with shape (810000,) and data type float32"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Rutas\n",
    "PREPROCESSED_PATH = r\"C:\\DAVID\\CS\\2025 0\\machine_learning\\Proyecto3\\P3_ML_GP6\\pre_processing\"\n",
    "LABELS_FILE = r\"C:\\DAVID\\CS\\2025 0\\machine_learning\\Data_Proyect3\\ISIC2018_Task3_Training_GroundTruth\\ISIC2018_Task3_Training_GroundTruth\\ISIC2018_Task3_Training_GroundTruth.csv\"\n",
    "OUTPUT_PATH = r\"C:\\DAVID\\CS\\2025 0\\machine_learning\\Proyecto3\\P3_ML_GP6\\reduced_data\"\n",
    "\n",
    "# Crear carpeta si no existe\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(\"üì• Cargando etiquetas...\")\n",
    "df = pd.read_csv(LABELS_FILE, sep=';')\n",
    "df['diagnosis'] = df.iloc[:, 1:].idxmax(axis=1)\n",
    "\n",
    "# Configurar tama√±o de lotes\n",
    "batch_size_pca = 250  # Reducimos el tama√±o del lote para PCA\n",
    "batch_size_lda = 100  # M√°s peque√±o para LDA\n",
    "num_batches_pca = len(df) // batch_size_pca + 1\n",
    "num_batches_lda = len(df) // batch_size_lda + 1\n",
    "\n",
    "# Definir IncrementalPCA y LDA\n",
    "pca = IncrementalPCA(n_components=50, batch_size=batch_size_pca)\n",
    "lda = LDA(n_components=6)\n",
    "\n",
    "print(\"‚öôÔ∏è Aplicando PCA en lotes con `IncrementalPCA`...\")\n",
    "\n",
    "# üîπ 1. Entrenar PCA en lotes peque√±os sin sobrecargar la RAM\n",
    "for i in range(num_batches_pca):\n",
    "    print(f\"üì¶ Procesando PCA - Lote {i+1}/{num_batches_pca}...\")\n",
    "\n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    for _, row in tqdm(df.iloc[i * batch_size_pca: (i + 1) * batch_size_pca].iterrows(), total=batch_size_pca):\n",
    "        image_name = row['image'] + \".npy\"\n",
    "        image_path = os.path.join(PREPROCESSED_PATH, image_name)\n",
    "\n",
    "        if os.path.exists(image_path):\n",
    "            img = np.load(image_path).flatten()\n",
    "            X_batch.append(img)\n",
    "            y_batch.append(row['diagnosis'])\n",
    "\n",
    "    if len(X_batch) == 0:\n",
    "        continue\n",
    "\n",
    "    X_batch = np.array(X_batch, dtype=np.float16)  # Convertimos a float16 para ahorrar memoria\n",
    "    y_batch = np.array(y_batch)\n",
    "\n",
    "    print(\"üîπ Ajustando PCA con este lote...\")\n",
    "    pca.partial_fit(X_batch)  # Usa `partial_fit()` en lugar de `fit()`\n",
    "\n",
    "print(\"‚úÖ PCA entrenado, guardando componentes...\")\n",
    "np.save(os.path.join(OUTPUT_PATH, \"pca_components.npy\"), pca.components_)\n",
    "\n",
    "# üîπ Aplicar transformaci√≥n PCA en lotes\n",
    "print(\"‚öôÔ∏è Aplicando transformaci√≥n PCA...\")\n",
    "for i in range(num_batches_pca):\n",
    "    print(f\"üì¶ Transformando PCA - Lote {i+1}/{num_batches_pca}...\")\n",
    "\n",
    "    X_batch = []\n",
    "    for _, row in tqdm(df.iloc[i * batch_size_pca: (i + 1) * batch_size_pca].iterrows(), total=batch_size_pca):\n",
    "        image_name = row['image'] + \".npy\"\n",
    "        image_path = os.path.join(PREPROCESSED_PATH, image_name)\n",
    "\n",
    "        if os.path.exists(image_path):\n",
    "            img = np.load(image_path).flatten()\n",
    "            X_batch.append(img)\n",
    "\n",
    "    if len(X_batch) == 0:\n",
    "        continue\n",
    "\n",
    "    X_batch = np.array(X_batch, dtype=np.float16)\n",
    "\n",
    "    print(\"üîπ Aplicando transformaci√≥n PCA...\")\n",
    "    X_pca_batch = pca.transform(X_batch)\n",
    "    np.save(os.path.join(OUTPUT_PATH, f\"pca_reduced_batch_{i}.npy\"), X_pca_batch)\n",
    "\n",
    "    print(f\"‚úÖ Lote {i+1}/{num_batches_pca} procesado y guardado para PCA.\")\n",
    "\n",
    "# üîπ 2. Procesar LDA en lotes m√°s peque√±os\n",
    "print(\"‚öôÔ∏è Aplicando LDA en lotes...\")\n",
    "first_batch_lda = True\n",
    "for i in range(num_batches_lda):\n",
    "    print(f\"üì¶ Procesando LDA - Lote {i+1}/{num_batches_lda}...\")\n",
    "\n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    for _, row in tqdm(df.iloc[i * batch_size_lda: (i + 1) * batch_size_lda].iterrows(), total=batch_size_lda):\n",
    "        image_name = row['image'] + \".npy\"\n",
    "        image_path = os.path.join(PREPROCESSED_PATH, image_name)\n",
    "\n",
    "        if os.path.exists(image_path):\n",
    "            img = np.load(image_path).flatten()\n",
    "            X_batch.append(img)\n",
    "            y_batch.append(row['diagnosis'])\n",
    "\n",
    "    if len(X_batch) == 0:\n",
    "        continue\n",
    "\n",
    "    X_batch = np.array(X_batch, dtype=np.float16)\n",
    "    y_batch = np.array(y_batch)\n",
    "\n",
    "    if first_batch_lda:\n",
    "        print(\"üîπ Entrenando LDA con primer lote...\")\n",
    "        lda.fit(X_batch, y_batch)\n",
    "        first_batch_lda = False\n",
    "\n",
    "    print(\"üîπ Aplicando transformaci√≥n LDA...\")\n",
    "    X_lda_batch = lda.transform(X_batch)\n",
    "    np.save(os.path.join(OUTPUT_PATH, f\"lda_reduced_batch_{i}.npy\"), X_lda_batch)\n",
    "\n",
    "    print(f\"‚úÖ Lote {i+1}/{num_batches_lda} procesado y guardado para LDA.\")\n",
    "\n",
    "print(\"üéâ Reducci√≥n de dimensionalidad completada.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
