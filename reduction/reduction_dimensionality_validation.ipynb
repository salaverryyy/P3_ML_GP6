{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduccion Dimensionalidad PCA y LDA para Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Cargando im√°genes originales de TRAIN para reentrenar PCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10015 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10015/10015 [05:52<00:00, 28.37it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 30.2 GiB for an array with shape (10015, 810000) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m archivos_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(PREPROCESSED_PATH_TRAIN) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[0;32m     19\u001b[0m X_train \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(PREPROCESSED_PATH_TRAIN, file))\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m tqdm(archivos_train)]\n\u001b[1;32m---> 20\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# üì• Entrenar PCA correctamente desde im√°genes originales\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚öôÔ∏è Entrenando PCA desde cero con im√°genes originales de TRAIN...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 30.2 GiB for an array with shape (10015, 810000) and data type float32"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# üìå Rutas\n",
    "PREPROCESSED_PATH_TRAIN = r\"C:\\DAVID\\CS\\2025 0\\machine_learning\\Proyecto3\\pequena_observacion_data\\pre_processing\"\n",
    "PREPROCESSED_PATH_VALIDATION = r\"C:\\DAVID\\CS\\2025 0\\machine_learning\\P3_ML_GP6\\pre_processing\\validation\"\n",
    "OUTPUT_PATH = r\"C:\\DAVID\\CS\\2025 0\\machine_learning\\P3_ML_GP6\\reduced_data\"\n",
    "\n",
    "# Crear carpeta si no existe\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# üì• Inicializar IncrementalPCA (procesa por lotes)\n",
    "pca = IncrementalPCA(n_components=50, batch_size=100)  # üîπ Procesa 100 im√°genes a la vez\n",
    "\n",
    "# üì• Entrenar PCA por lotes en `train`\n",
    "print(\"‚öôÔ∏è Entrenando IncrementalPCA por lotes con im√°genes originales de TRAIN...\")\n",
    "archivos_train = sorted([f for f in os.listdir(PREPROCESSED_PATH_TRAIN) if f.endswith(\".npy\")])\n",
    "\n",
    "for i in tqdm(range(0, len(archivos_train), 100)):  # üîπ Procesa en lotes de 100 im√°genes\n",
    "    batch_files = archivos_train[i:i + 100]\n",
    "    X_batch = [np.load(os.path.join(PREPROCESSED_PATH_TRAIN, file)).flatten() for file in batch_files]\n",
    "    X_batch = np.vstack(X_batch)\n",
    "    pca.partial_fit(X_batch)  # üîπ Usa `partial_fit()` en lugar de `fit()`\n",
    "\n",
    "# üíæ Guardar el modelo PCA entrenado\n",
    "np.save(os.path.join(OUTPUT_PATH, \"pca_components.npy\"), pca.components_)\n",
    "np.save(os.path.join(OUTPUT_PATH, \"pca_mean.npy\"), pca.mean_)\n",
    "\n",
    "# üì• Cargar y entrenar LDA con `train` (tambi√©n por lotes)\n",
    "lda = LDA(n_components=6)\n",
    "X_train_pca = []\n",
    "\n",
    "print(\"‚öôÔ∏è Aplicando PCA a TRAIN para entrenar LDA...\")\n",
    "for i in tqdm(range(0, len(archivos_train), 100)):\n",
    "    batch_files = archivos_train[i:i + 100]\n",
    "    X_batch = [np.load(os.path.join(PREPROCESSED_PATH_TRAIN, file)).flatten() for file in batch_files]\n",
    "    X_batch = np.vstack(X_batch)\n",
    "    X_train_pca.append(pca.transform(X_batch))\n",
    "\n",
    "X_train_pca = np.vstack(X_train_pca)\n",
    "y_train = np.load(os.path.join(OUTPUT_PATH, \"y_train.npy\"), allow_pickle=True)\n",
    "lda.fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"‚öôÔ∏è Aplicando PCA y LDA a Validation...\")\n",
    "\n",
    "# üìÇ Obtener la lista de im√°genes preprocesadas de `validation`\n",
    "archivos_val = sorted([f for f in os.listdir(PREPROCESSED_PATH_VALIDATION) if f.endswith(\".npy\")])\n",
    "\n",
    "X_reduced_pca = []\n",
    "X_reduced_lda = []\n",
    "\n",
    "for file in tqdm(archivos_val):\n",
    "    image_path = os.path.join(PREPROCESSED_PATH_VALIDATION, file)\n",
    "    img = np.load(image_path).flatten()\n",
    "\n",
    "    # Aplicar PCA correctamente\n",
    "    img_pca = pca.transform(img.reshape(1, -1))\n",
    "    X_reduced_pca.append(img_pca)\n",
    "\n",
    "    # Aplicar LDA\n",
    "    img_lda = lda.transform(img_pca)\n",
    "    X_reduced_lda.append(img_lda)\n",
    "\n",
    "# üì• Convertir listas a arrays numpy\n",
    "X_reduced_pca = np.vstack(X_reduced_pca)\n",
    "X_reduced_lda = np.vstack(X_reduced_lda)\n",
    "\n",
    "# üíæ Guardar los archivos reducidos\n",
    "np.save(os.path.join(OUTPUT_PATH, \"pca_reduced_validation.npy\"), X_reduced_pca)\n",
    "np.save(os.path.join(OUTPUT_PATH, \"lda_reduced_validation.npy\"), X_reduced_lda)\n",
    "\n",
    "print(\"‚úÖ Reducci√≥n de dimensionalidad para Validation completada.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
